{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наивные байесовские классификаторы представляют собой семейство классификаторов, которые очень схожи с линейными моделями, рассмотренными в предыдущем разделе. Однако они имеют тенденцию обучаться быстрее. Цена, которую приходится платить за такую эффективность –немного более низкая обобщающая способность моделей Байеса по сравнению с линейными классификаторами типа LogisticRegression и LinearSVC\n",
    "\n",
    "Причина, по которой наивные байесовские модели столь эффективны, заключается в том, что они оценивают параметры, рассматривая каждый признак отдельно и по каждому признаку собирают простые статистики классов. В scikit-learn реализованы три вида наивных байесовских классификаторов: GaussianNB, BernoulliNBи MultinomialNB\n",
    "\n",
    "GaussianNBможно применить к любым непрерывным данным, в то время как BernoulliNBпринимает бинарные данные, MultinomialNBпринимает счетные или дискретные данные (то есть каждый признак представляет собой подсчет целочисленных значений какой-то характеристики, например, речь может идти о частоте встречаемости слова в предложении). BernoulliNBи MultinomialNBв основном используются для классификации текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частоты признаков:\n",
      "{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n",
      "clf.predict:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1], [1, 0, 1, 0]])\n",
    "Y = np.array([0, 1, 0, 1])\n",
    "counts = {}\n",
    "for label in np.unique(Y):\n",
    "   counts[label] = X[Y == label].sum(axis=0)\n",
    "print(\"Частоты признаков:\\n{}\".format(counts))\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, Y)\n",
    "print(\"clf.predict:\\n{}\".format(clf.predict(X[2:3])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB\n",
    "\n",
    "можно применить к любым непрерывным данным\n",
    "\n",
    "записывает среднее значение, а также стандартное отклонение каждого признака для каждого класса\n",
    "\n",
    "BernoulliNB\n",
    "\n",
    "принимает бинарные данные\n",
    "\n",
    "для классификации текстовых данных\n",
    "\n",
    "одсчитывает ненулевые частоты признаков по каждому классу.\n",
    "\n",
    "MultinomialNB\n",
    "\n",
    "принимает счетные или дискретные данные\n",
    "\n",
    "для классификации текстовых данных\n",
    "\n",
    "принимает в расчет среднее значение каждого признака для каждого класс"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор BernoulliNBподсчитывает ненулевые частоты признаков по каждому классу. \n",
    "\n",
    "Здесь у нас есть четыре точки данных с четырьмя бинарными признаками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "Y = np.array([1, 2, 3, 4, 5, 6])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict(X[2:3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "print(clf_pf.predict([[-0.8, -1]]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две другие наивные байесовские модели MultinomialNBи GaussianNB, немного отличаются с  точки  зрения  вычисляемых  статистик. MultinomialNBпринимает  в  расчет  среднее  значение каждого признака для каждого класса, в то время как GaussianNBзаписывает среднее значение, а также стандартное отклонение каждого признака для каждого класса\n",
    "\n",
    "Для получения прогноза точка данных сравнивается со статистиками для каждого класса и прогнозируется  наиболее  подходящий  класс.  Интересно  отметить,  что  для MultinomialNBи BernoulliNBэто приводитпрогнозной формуле, которая имеет точно такойже вид, что и формула для линейных моделей (см. «Линейные модели классификации»). К сожалению, coef_для наивных байесовских моделей имеет несколько иной смысл, чем coef_для линейных моделей, здесь coef_не тождественен w\n",
    "\n",
    "MultinomialNBи BernoulliNBимеют один параметр alpha, который контролирует сложность модели. Параметр alphaработает следующим образом: алгоритм добавляет к данным зависящее от alphaопределенное количество искусственных наблюдений с положительными значениями для всех признаков. Это приводит к «сглаживанию» статистик. Большее значение alphaозначает более высокую  степень  сглаживания,  что  приводит  к  построению  менее  сложных  моделей.  Алгоритм относительно устойчив к разным значениям alpha. Это означает, что значение alphaне оказывает значительного влияния на хорошую работу модели. Вместе с тем тонкая настройка этого параметра обычно немного увеличивает правильност\n",
    "\n",
    "GaussianNBв основном используется для данных с очень высокой размерностью, тогда как остальные  наивные  байесовские  моделишироко  используются  для  разреженных  дискретных данных, например, для текста. MultinomialNBобычно работает лучше, чем BernoulliNB, особенно на наборах данных с относительно большим количеством признаков, имеющих ненулевые частоты (т.е. на больших документах)\n",
    "\n",
    "Они  очень  быстро  обучаются  и  прогнозируют,  а  процесс  обучения  легко интерпретировать. Модели очень хорошо работают с высокоразмерными разреженными данными и относительно  устойчивы  к  изменениям  параметров.  Наивные  байесовские  модели  являются замечательными базовыми моделями и часто используются на очень больших наборахданных, где обучение даже линейной модели может занять слишком много времени"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df7328e814b316b1e1a17499c8ee6d0f88fbfa68f71091a50220b004c68ad64d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
